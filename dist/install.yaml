## ATTN KUBERNETES ADMINS! Read this...
#  If prometheus-server is already installed, and you want to use that version,
#  comment out the section from "START PROM SERVER" to "END PROM SERVER" and update the "prometheusURL" variable.
#
#  If prometheus-node-exporter is already installed, and you want to use that version,
#  comment out the section from "START PROM NODE EXPORTER" to "END PROM NODE EXPORTER"
# 

apiVersion: v1
kind: Namespace
metadata:
  labels:
    control-plane: controller-manager
    app.kubernetes.io/name: devzero-zxporter
  name: devzero-zxporter
# ----- START PROM SERVER -----
---
# Source: prometheus/charts/kube-state-metrics/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-5.15.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/version: "2.10.1"
  name: prometheus-kube-state-metrics
  namespace: devzero-zxporter
imagePullSecrets:
---
# Source: prometheus/charts/prometheus-node-exporter/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus-prometheus-node-exporter
  namespace: devzero-zxporter
  labels:
    helm.sh/chart: prometheus-node-exporter-4.24.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/version: "1.7.0"
---
# Source: prometheus/charts/prometheus-pushgateway/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: prometheus-pushgateway-2.4.2
    app.kubernetes.io/name: prometheus-pushgateway
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/version: "v1.6.2"
    app.kubernetes.io/managed-by: Helm
  name: prometheus-prometheus-pushgateway
  namespace: devzero-zxporter
---
# Source: prometheus/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/version: v2.48.0
    helm.sh/chart: prometheus-25.8.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: prometheus
  name: prometheus-server
  namespace: devzero-zxporter
  annotations:
    {}
---
# Source: prometheus/templates/cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/version: v2.48.0
    helm.sh/chart: prometheus-25.8.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: prometheus
  name: prometheus-server
  namespace: devzero-zxporter
data:
  allow-snippet-annotations: "false"
  alerting_rules.yml: |
    {}
  alerts: |
    {}
  prometheus.yml: |
    global:
      evaluation_interval: 1m
      scrape_interval: 1m
      scrape_timeout: 10s
    rule_files:
    - /etc/config/recording_rules.yml
    - /etc/config/alerting_rules.yml
    - /etc/config/rules
    - /etc/config/alerts
    scrape_configs:
    - job_name: prometheus
      static_configs:
      - targets:
        - localhost:9090
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-apiservers
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: default;kubernetes;https
        source_labels:
        - __meta_kubernetes_namespace
        - __meta_kubernetes_service_name
        - __meta_kubernetes_endpoint_port_name
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-nodes
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: kubernetes.default.svc:443
        target_label: __address__
      - regex: (.+)
        replacement: /api/v1/nodes/$1/proxy/metrics
        source_labels:
        - __meta_kubernetes_node_name
        target_label: __metrics_path__
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-nodes-cadvisor
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: kubernetes.default.svc:443
        target_label: __address__
      - regex: (.+)
        replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor
        source_labels:
        - __meta_kubernetes_node_name
        target_label: __metrics_path__
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - honor_labels: true
      job_name: kubernetes-service-endpoints
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape
      - action: drop
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: (.+?)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_service_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
        replacement: __param_$1
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_service_name
        target_label: service
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: node
    - honor_labels: true
      job_name: kubernetes-service-endpoints-slow
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: (.+?)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_service_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
        replacement: __param_$1
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_service_name
        target_label: service
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: node
      scrape_interval: 5m
      scrape_timeout: 30s
    - honor_labels: true
      job_name: prometheus-pushgateway
      kubernetes_sd_configs:
      - role: service
      relabel_configs:
      - action: keep
        regex: pushgateway
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_probe
    - honor_labels: true
      job_name: kubernetes-services
      kubernetes_sd_configs:
      - role: service
      metrics_path: /probe
      params:
        module:
        - http_2xx
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_probe
      - source_labels:
        - __address__
        target_label: __param_target
      - replacement: blackbox
        target_label: __address__
      - source_labels:
        - __param_target
        target_label: instance
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - source_labels:
        - __meta_kubernetes_service_name
        target_label: service
    - honor_labels: true
      job_name: kubernetes-pods
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape
      - action: drop
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: (\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})
        replacement: '[$2]:$1'
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        - __meta_kubernetes_pod_ip
        target_label: __address__
      - action: replace
        regex: (\d+);((([0-9]+?)(\.|$)){4})
        replacement: $2:$1
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        - __meta_kubernetes_pod_ip
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
        replacement: __param_$1
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: pod
      - action: drop
        regex: Pending|Succeeded|Failed|Completed
        source_labels:
        - __meta_kubernetes_pod_phase
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: node
    - honor_labels: true
      job_name: kubernetes-pods-slow
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: (\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})
        replacement: '[$2]:$1'
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        - __meta_kubernetes_pod_ip
        target_label: __address__
      - action: replace
        regex: (\d+);((([0-9]+?)(\.|$)){4})
        replacement: $2:$1
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        - __meta_kubernetes_pod_ip
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
        replacement: __param_$1
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: pod
      - action: drop
        regex: Pending|Succeeded|Failed|Completed
        source_labels:
        - __meta_kubernetes_pod_phase
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: node
      scrape_interval: 5m
      scrape_timeout: 30s
  recording_rules.yml: |
    {}
  rules: |
    {}
---
# Source: prometheus/charts/kube-state-metrics/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-5.15.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/version: "2.10.1"
  name: prometheus-kube-state-metrics
rules:

- apiGroups: ["certificates.k8s.io"]
  resources:
  - certificatesigningrequests
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - cronjobs
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - daemonsets
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - deployments
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - endpoints
  verbs: ["list", "watch"]

- apiGroups: ["autoscaling"]
  resources:
  - horizontalpodautoscalers
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "networking.k8s.io"]
  resources:
  - ingresses
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - jobs
  verbs: ["list", "watch"]

- apiGroups: ["coordination.k8s.io"]
  resources:
  - leases
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - limitranges
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - mutatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - namespaces
  verbs: ["list", "watch"]

- apiGroups: ["networking.k8s.io"]
  resources:
  - networkpolicies
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - nodes
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumeclaims
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumes
  verbs: ["list", "watch"]

- apiGroups: ["policy"]
  resources:
    - poddisruptionbudgets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - pods
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - replicasets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - replicationcontrollers
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - resourcequotas
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - secrets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - services
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - statefulsets
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - storageclasses
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - validatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - volumeattachments
  verbs: ["list", "watch"]
---
# Source: prometheus/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/version: v2.48.0
    helm.sh/chart: prometheus-25.8.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: prometheus
  name: prometheus-server
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
      - nodes/proxy
      - nodes/metrics
      - services
      - endpoints
      - pods
      - ingresses
      - configmaps
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses/status
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "discovery.k8s.io"
    resources:
      - endpointslices
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - "/metrics"
    verbs:
      - get
---
# Source: prometheus/charts/kube-state-metrics/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-5.15.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/version: "2.10.1"
  name: prometheus-kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus-kube-state-metrics
subjects:
- kind: ServiceAccount
  name: prometheus-kube-state-metrics
  namespace: devzero-zxporter
---
# Source: prometheus/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/version: v2.48.0
    helm.sh/chart: prometheus-25.8.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: prometheus
  name: prometheus-server
subjects:
  - kind: ServiceAccount
    name: prometheus-server
    namespace: devzero-zxporter
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus-server
---
# Source: prometheus/charts/kube-state-metrics/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: prometheus-kube-state-metrics
  namespace: devzero-zxporter
  labels:    
    helm.sh/chart: kube-state-metrics-5.15.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/version: "2.10.1"
  annotations:
    prometheus.io/scrape: 'true'
spec:
  type: "ClusterIP"
  ports:
  - name: "http"
    protocol: TCP
    port: 8080
    targetPort: 8080
  
  selector:    
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: prometheus
---
# Source: prometheus/charts/prometheus-node-exporter/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: prometheus-prometheus-node-exporter
  namespace: devzero-zxporter
  labels:
    helm.sh/chart: prometheus-node-exporter-4.24.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/version: "1.7.0"
  annotations:
    prometheus.io/scrape: "true"
spec:
  type: ClusterIP
  ports:
    - port: 9100
      targetPort: 9100
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: prometheus
---
# Source: prometheus/charts/prometheus-pushgateway/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/probe: pushgateway
  labels:
    helm.sh/chart: prometheus-pushgateway-2.4.2
    app.kubernetes.io/name: prometheus-pushgateway
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/version: "v1.6.2"
    app.kubernetes.io/managed-by: Helm
  name: prometheus-prometheus-pushgateway
  namespace: devzero-zxporter
spec:
  type: ClusterIP
  ports:
    - port: 9091
      targetPort: 9091
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: prometheus-pushgateway
    app.kubernetes.io/instance: prometheus
---
# Source: prometheus/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/version: v2.48.0
    helm.sh/chart: prometheus-25.8.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: prometheus
  name: prometheus-server
  namespace: devzero-zxporter
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 9090
  selector:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: prometheus
  sessionAffinity: None
  type: "ClusterIP"
---
# Source: prometheus/charts/prometheus-node-exporter/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: prometheus-prometheus-node-exporter
  namespace: devzero-zxporter
  labels:
    helm.sh/chart: prometheus-node-exporter-4.24.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/version: "1.7.0"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/instance: prometheus
  revisionHistoryLimit: 10
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        helm.sh/chart: prometheus-node-exporter-4.24.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: prometheus-node-exporter
        app.kubernetes.io/name: prometheus-node-exporter
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/version: "1.7.0"
    spec:
      automountServiceAccountToken: false
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      serviceAccountName: prometheus-prometheus-node-exporter
      containers:
        - name: node-exporter
          image: quay.io/prometheus/node-exporter:v1.7.0
          imagePullPolicy: IfNotPresent
          args:
            - --path.procfs=/host/proc
            - --path.sysfs=/host/sys
            - --path.rootfs=/host/root
            - --path.udev.data=/host/root/run/udev/data
            - --web.listen-address=[$(HOST_IP)]:9100
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
          env:
            - name: HOST_IP
              value: 0.0.0.0
          ports:
            - name: metrics
              containerPort: 9100
              protocol: TCP
          livenessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /
              port: 9100
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /
              port: 9100
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          volumeMounts:
            - name: proc
              mountPath: /host/proc
              readOnly:  true
            - name: sys
              mountPath: /host/sys
              readOnly: true
            - name: root
              mountPath: /host/root
              mountPropagation: HostToContainer
              readOnly: true
      hostNetwork: true
      hostPID: true
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
        - name: root
          hostPath:
            path: /
---
# Source: prometheus/charts/kube-state-metrics/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus-kube-state-metrics
  namespace: devzero-zxporter
  labels:    
    helm.sh/chart: kube-state-metrics-5.15.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/version: "2.10.1"
spec:
  selector:
    matchLabels:      
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/instance: prometheus
  replicas: 1
  strategy:
    type: RollingUpdate
  revisionHistoryLimit: 10
  template:
    metadata:
      labels:        
        helm.sh/chart: kube-state-metrics-5.15.2
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: kube-state-metrics
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/version: "2.10.1"
    spec:
      hostNetwork: false
      serviceAccountName: prometheus-kube-state-metrics
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: kube-state-metrics
        args:
        - --port=8080
        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
        imagePullPolicy: IfNotPresent
        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1
        ports:
        - containerPort: 8080
          name: "http"
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
---
# Source: prometheus/charts/prometheus-pushgateway/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    helm.sh/chart: prometheus-pushgateway-2.4.2
    app.kubernetes.io/name: prometheus-pushgateway
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/version: "v1.6.2"
    app.kubernetes.io/managed-by: Helm
  name: prometheus-prometheus-pushgateway
  namespace: devzero-zxporter
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/instance: prometheus
  template:
    metadata:
      labels:
        helm.sh/chart: prometheus-pushgateway-2.4.2
        app.kubernetes.io/name: prometheus-pushgateway
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/version: "v1.6.2"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: prometheus-prometheus-pushgateway
      containers:
        - name: pushgateway
          image: "quay.io/prometheus/pushgateway:v1.6.2"
          imagePullPolicy: IfNotPresent
          ports:
            - name: metrics
              containerPort: 9091
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9091
            initialDelaySeconds: 10
            timeoutSeconds: 10
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9091
            initialDelaySeconds: 10
            timeoutSeconds: 10
          volumeMounts:
            - name: storage-volume
              mountPath: "/data"
              subPath: ""
      securityContext:
        fsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      volumes:
        - name: storage-volume
          emptyDir: {}
---
# Source: prometheus/templates/deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/version: v2.48.0
    helm.sh/chart: prometheus-25.8.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: prometheus
  name: prometheus-server
  namespace: devzero-zxporter
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: server
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/instance: prometheus
  replicas: 1
  revisionHistoryLimit: 10
  strategy:
    type: Recreate
    rollingUpdate: null
  template:
    metadata:
      labels:
        app.kubernetes.io/component: server
        app.kubernetes.io/name: prometheus
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/version: v2.48.0
        helm.sh/chart: prometheus-25.8.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/part-of: prometheus
    spec:
      enableServiceLinks: true
      serviceAccountName: prometheus-server
      containers:
        - name: prometheus-server-configmap-reload
          image: "quay.io/prometheus-operator/prometheus-config-reloader:v0.67.0"
          imagePullPolicy: "IfNotPresent"
          args:
            - --watched-dir=/etc/config
            - --reload-url=http://127.0.0.1:9090/-/reload
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
              readOnly: true

        - name: prometheus-server
          image: "quay.io/prometheus/prometheus:v2.48.0"
          imagePullPolicy: "IfNotPresent"
          args:
            - --storage.tsdb.retention.time=15d
            - --config.file=/etc/config/prometheus.yml
            - --storage.tsdb.path=/data
            - --web.console.libraries=/etc/prometheus/console_libraries
            - --web.console.templates=/etc/prometheus/consoles
            - --web.enable-lifecycle
          ports:
            - containerPort: 9090
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            timeoutSeconds: 4
            failureThreshold: 3
            successThreshold: 1
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            timeoutSeconds: 10
            failureThreshold: 3
            successThreshold: 1
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
            - name: storage-volume
              mountPath: /data
              subPath: ""
      dnsPolicy: ClusterFirst
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      terminationGracePeriodSeconds: 300
      volumes:
        - name: config-volume
          configMap:
            name: prometheus-server
        - name: storage-volume
          emptyDir:
            {}
# ----- END PROM SERVER -----
# ----- START PROM NODE EXPORTER -----
---
# Source: prometheus-node-exporter/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: node-exporter-prometheus-node-exporter
  namespace: devzero-zxporter
  labels:
    helm.sh/chart: prometheus-node-exporter-4.24.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: node-exporter
    app.kubernetes.io/version: "1.7.0"
---
# Source: prometheus-node-exporter/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: node-exporter-prometheus-node-exporter
  namespace: devzero-zxporter
  labels:
    helm.sh/chart: prometheus-node-exporter-4.24.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: node-exporter
    app.kubernetes.io/version: "1.7.0"
  annotations:
    prometheus.io/scrape: "true"
spec:
  type: ClusterIP
  ports:
    - port: 9100
      targetPort: 9100
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: node-exporter
---
# Source: prometheus-node-exporter/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter-prometheus-node-exporter
  namespace: devzero-zxporter
  labels:
    helm.sh/chart: prometheus-node-exporter-4.24.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: node-exporter
    app.kubernetes.io/version: "1.7.0"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/instance: node-exporter
  revisionHistoryLimit: 10
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        helm.sh/chart: prometheus-node-exporter-4.24.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: prometheus-node-exporter
        app.kubernetes.io/name: prometheus-node-exporter
        app.kubernetes.io/instance: node-exporter
        app.kubernetes.io/version: "1.7.0"
    spec:
      automountServiceAccountToken: false
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      serviceAccountName: node-exporter-prometheus-node-exporter
      containers:
        - name: node-exporter
          image: quay.io/prometheus/node-exporter:v1.7.0
          imagePullPolicy: IfNotPresent
          args:
            - --path.procfs=/host/proc
            - --path.sysfs=/host/sys
            - --path.rootfs=/host/root
            - --path.udev.data=/host/root/run/udev/data
            - --web.listen-address=[$(HOST_IP)]:9100
          securityContext:
            readOnlyRootFilesystem: true
          env:
            - name: HOST_IP
              value: 0.0.0.0
          ports:
            - name: metrics
              containerPort: 9100
              protocol: TCP
          livenessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /
              port: 9100
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /
              port: 9100
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          volumeMounts:
            - name: proc
              mountPath: /host/proc
              readOnly:  true
            - name: sys
              mountPath: /host/sys
              readOnly: true
            - name: root
              mountPath: /host/root
              mountPropagation: HostToContainer
              readOnly: true
      hostNetwork: true
      hostPID: true
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
        - name: root
          hostPath:
            path: /
# ----- END PROM NODE EXPORTER -----
# ----- START NVIDIA GPU OPERATOR -----
# Note: Comment out this section if gpu-operator is already installed
---
# Source: gpu-operator/charts/node-feature-discovery/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: node-feature-discovery
  namespace: default
  labels:
    helm.sh/chart: node-feature-discovery-0.16.6
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.16.6"
    app.kubernetes.io/managed-by: Helm
---
# Source: gpu-operator/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: gpu-operator
  labels:
    app.kubernetes.io/name: gpu-operator
    helm.sh/chart: gpu-operator-v24.9.2
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v24.9.2"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: "gpu-operator"
---
# Source: gpu-operator/charts/node-feature-discovery/templates/nfd-master-conf.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: gpu-operator-node-feature-discovery-master-conf
  namespace: default
  labels:
    helm.sh/chart: node-feature-discovery-0.16.6
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.16.6"
    app.kubernetes.io/managed-by: Helm
data:
  nfd-master.conf: |-
    extraLabelNs:
    - nvidia.com
---
# Source: gpu-operator/charts/node-feature-discovery/templates/nfd-worker-conf.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: gpu-operator-node-feature-discovery-worker-conf
  namespace: default
  labels:
    helm.sh/chart: node-feature-discovery-0.16.6
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.16.6"
    app.kubernetes.io/managed-by: Helm
data:
  nfd-worker.conf: |-
    sources:
      pci:
        deviceClassWhitelist:
        - "02"
        - "0200"
        - "0207"
        - "0300"
        - "0302"
        deviceLabelFields:
        - vendor
---
# Source: gpu-operator/charts/node-feature-discovery/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: gpu-operator-node-feature-discovery
  labels:
    helm.sh/chart: node-feature-discovery-0.16.6
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.16.6"
    app.kubernetes.io/managed-by: Helm
rules:
- apiGroups:
  - ""
  resources:
  - nodes
  - nodes/status
  verbs:
  - get
  - patch
  - update
  - list
- apiGroups:
  - nfd.k8s-sigs.io
  resources:
  - nodefeatures
  - nodefeaturerules
  - nodefeaturegroups
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - nfd.k8s-sigs.io
  resources:
  - nodefeaturegroups/status
  verbs:
  - patch
  - update
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - create
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  resourceNames:
  - "nfd-master.nfd.kubernetes.io"
  verbs:
  - get
  - update
---
# Source: gpu-operator/charts/node-feature-discovery/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: gpu-operator-node-feature-discovery-gc
  labels:
    helm.sh/chart: node-feature-discovery-0.16.6
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.16.6"
    app.kubernetes.io/managed-by: Helm
rules:
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes/proxy
  verbs:
  - get
- apiGroups:
  - topology.node.k8s.io
  resources:
  - noderesourcetopologies
  verbs:
  - delete
  - list
- apiGroups:
  - nfd.k8s-sigs.io
  resources:
  - nodefeatures
  verbs:
  - delete
  - list
---
# Source: gpu-operator/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: gpu-operator
  labels:
    app.kubernetes.io/name: gpu-operator
    helm.sh/chart: gpu-operator-v24.9.2
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v24.9.2"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: "gpu-operator"
rules:
- apiGroups:
  - config.openshift.io
  resources:
  - clusterversions
  - proxies
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - image.openshift.io
  resources:
  - imagestreams
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - security.openshift.io
  resources:
  - securitycontextconstraints
  verbs:
  - create
  - get
  - list
  - watch
  - update
  - patch
  - delete
  - use
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - clusterroles
  - clusterrolebindings
  verbs:
  - create
  - get
  - list
  - watch
  - update
  - patch
  - delete
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
  - list
  - watch
  - update
  - patch
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
  - list
  - create
  - watch
  - update
  - patch
- apiGroups:
  - ""
  resources:
  - events
  - pods
  - pods/eviction
  verbs:
  - create
  - get
  - list
  - watch
  - update
  - patch
  - delete
- apiGroups:
  - apps
  resources:
  - daemonsets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - nvidia.com
  resources:
  - clusterpolicies
  - clusterpolicies/finalizers
  - clusterpolicies/status
  - nvidiadrivers
  - nvidiadrivers/finalizers
  - nvidiadrivers/status
  verbs:
  - create
  - get
  - list
  - watch
  - update
  - patch
  - delete
  - deletecollection
- apiGroups:
  - scheduling.k8s.io
  resources:
  - priorityclasses
  verbs:
  - get
  - list
  - watch
  - create
- apiGroups:
  - node.k8s.io
  resources:
  - runtimeclasses
  verbs:
  - get
  - list
  - create
  - update
  - watch
  - delete
- apiGroups:
  - apiextensions.k8s.io
  resources:
  - customresourcedefinitions
  verbs:
  - get
  - list
  - watch
  - update
  - patch
  - create
---
# Source: gpu-operator/charts/node-feature-discovery/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: gpu-operator-node-feature-discovery
  labels:
    helm.sh/chart: node-feature-discovery-0.16.6
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.16.6"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: gpu-operator-node-feature-discovery
subjects:
- kind: ServiceAccount
  name: node-feature-discovery
  namespace: default
---
# Source: gpu-operator/charts/node-feature-discovery/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: gpu-operator-node-feature-discovery-gc
  labels:
    helm.sh/chart: node-feature-discovery-0.16.6
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.16.6"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: gpu-operator-node-feature-discovery-gc
subjects:
- kind: ServiceAccount
  name: node-feature-discovery
  namespace: default
---
# Source: gpu-operator/templates/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: gpu-operator
  labels:
    app.kubernetes.io/name: gpu-operator
    helm.sh/chart: gpu-operator-v24.9.2
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v24.9.2"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: "gpu-operator"
subjects:
- kind: ServiceAccount
  name: gpu-operator
  namespace: default
- kind: ServiceAccount
  name: node-feature-discovery
  namespace: default
roleRef:
  kind: ClusterRole
  name: gpu-operator
  apiGroup: rbac.authorization.k8s.io
---
# Source: gpu-operator/charts/node-feature-discovery/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: gpu-operator-node-feature-discovery-worker
  namespace: default
  labels:
    helm.sh/chart: node-feature-discovery-0.16.6
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.16.6"
    app.kubernetes.io/managed-by: Helm
rules:
- apiGroups:
  - nfd.k8s-sigs.io
  resources:
  - nodefeatures
  verbs:
  - create
  - get
  - update
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
---
# Source: gpu-operator/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: gpu-operator
  labels:
    app.kubernetes.io/name: gpu-operator
    helm.sh/chart: gpu-operator-v24.9.2
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v24.9.2"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: "gpu-operator"
rules:
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - roles
  - rolebindings
  verbs:
  - create
  - get
  - list
  - watch
  - update
  - patch
  - delete
- apiGroups:
  - apps
  resources:
  - controllerrevisions
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - daemonsets
  verbs:
  - create
  - get
  - list
  - watch
  - update
  - patch
  - delete
- apiGroups:
  - ""
  resources:
  - configmaps
  - endpoints
  - pods
  - pods/eviction
  - secrets
  - services
  - services/finalizers
  - serviceaccounts
  verbs:
  - create
  - get
  - list
  - watch
  - update
  - patch
  - delete
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - monitoring.coreos.com
  resources:
  - servicemonitors
  - prometheusrules
  verbs:
  - get
  - list
  - create
  - watch
  - update
  - delete
---
# Source: gpu-operator/charts/node-feature-discovery/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: gpu-operator-node-feature-discovery-worker
  namespace: default
  labels:
    helm.sh/chart: node-feature-discovery-0.16.6
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.16.6"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: gpu-operator-node-feature-discovery-worker
subjects:
- kind: ServiceAccount
  name: node-feature-discovery
  namespace: default
---
# Source: gpu-operator/templates/rolebinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: gpu-operator
  labels:
    app.kubernetes.io/name: gpu-operator
    helm.sh/chart: gpu-operator-v24.9.2
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v24.9.2"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: "gpu-operator"
subjects:
- kind: ServiceAccount
  name: gpu-operator
  namespace: default
roleRef:
  kind: Role
  name: gpu-operator
  apiGroup: rbac.authorization.k8s.io
---
# Source: gpu-operator/charts/node-feature-discovery/templates/worker.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name:  gpu-operator-node-feature-discovery-worker
  namespace: default
  labels:
    helm.sh/chart: node-feature-discovery-0.16.6
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.16.6"
    app.kubernetes.io/managed-by: Helm
    role: worker
spec:
  revisionHistoryLimit: 
  selector:
    matchLabels:
      app.kubernetes.io/name: node-feature-discovery
      app.kubernetes.io/instance: gpu-operator
      role: worker
  template:
    metadata:
      labels:
        app.kubernetes.io/name: node-feature-discovery
        app.kubernetes.io/instance: gpu-operator
        role: worker
    spec:
      dnsPolicy: ClusterFirstWithHostNet
      priorityClassName: system-node-critical
      serviceAccountName: node-feature-discovery
      securityContext:
        {}
      hostNetwork: false
      containers:
      - name: worker
        securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
        image: "registry.k8s.io/nfd/node-feature-discovery:v0.16.6"
        imagePullPolicy: IfNotPresent
        livenessProbe:
            grpc:
              port: 8082
            initialDelaySeconds: 10
        readinessProbe:
            failureThreshold: 10
            grpc:
              port: 8082
            initialDelaySeconds: 5
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_UID
          valueFrom:
            fieldRef:
              fieldPath: metadata.uid
        resources:
            limits:
              memory: 512Mi
            requests:
              cpu: 5m
              memory: 64Mi
        command:
        - "nfd-worker"
        args:
# Go over featureGate and add the feature-gate flag
        - "-feature-gates=NodeFeatureAPI=true"
        - "-feature-gates=NodeFeatureGroupAPI=false"
        - "-metrics=8081"
        - "-grpc-health=8082"
        ports:
          - containerPort: 8081
            name: metrics
          - containerPort: 8082
            name: health
        volumeMounts:
        - name: host-boot
          mountPath: "/host-boot"
          readOnly: true
        - name: host-os-release
          mountPath: "/host-etc/os-release"
          readOnly: true
        - name: host-sys
          mountPath: "/host-sys"
          readOnly: true
        - name: host-usr-lib
          mountPath: "/host-usr/lib"
          readOnly: true
        - name: host-lib
          mountPath: "/host-lib"
          readOnly: true
        - name: host-proc-swaps
          mountPath: "/host-proc/swaps"
          readOnly: true
        - name: source-d
          mountPath: "/etc/kubernetes/node-feature-discovery/source.d/"
          readOnly: true
        - name: features-d
          mountPath: "/etc/kubernetes/node-feature-discovery/features.d/"
          readOnly: true
        - name: nfd-worker-conf
          mountPath: "/etc/kubernetes/node-feature-discovery"
          readOnly: true
      volumes:
        - name: host-boot
          hostPath:
            path: "/boot"
        - name: host-os-release
          hostPath:
            path: "/etc/os-release"
        - name: host-sys
          hostPath:
            path: "/sys"
        - name: host-usr-lib
          hostPath:
            path: "/usr/lib"
        - name: host-lib
          hostPath:
            path: "/lib"
        - name: host-proc-swaps
          hostPath:
            path: "/proc/swaps"
        - name: source-d
          hostPath:
            path: "/etc/kubernetes/node-feature-discovery/source.d/"
        - name: features-d
          hostPath:
            path: "/etc/kubernetes/node-feature-discovery/features.d/"
        - name: nfd-worker-conf
          configMap:
            name: gpu-operator-node-feature-discovery-worker-conf
            items:
              - key: nfd-worker.conf
                path: nfd-worker.conf
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Equal
          value: ""
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Equal
          value: ""
        - effect: NoSchedule
          key: nvidia.com/gpu
          operator: Exists
---
# Source: gpu-operator/charts/node-feature-discovery/templates/master.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name:  gpu-operator-node-feature-discovery-master
  namespace: default
  labels:
    helm.sh/chart: node-feature-discovery-0.16.6
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.16.6"
    app.kubernetes.io/managed-by: Helm
    role: master
spec:
  replicas: 1
  revisionHistoryLimit: 
  selector:
    matchLabels:
      app.kubernetes.io/name: node-feature-discovery
      app.kubernetes.io/instance: gpu-operator
      role: master
  template:
    metadata:
      labels:
        app.kubernetes.io/name: node-feature-discovery
        app.kubernetes.io/instance: gpu-operator
        role: master
    spec:
      priorityClassName: system-node-critical
      serviceAccountName: node-feature-discovery
      enableServiceLinks: false
      securityContext:
        {}
      hostNetwork: false
      containers:
        - name: master
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
          image: "registry.k8s.io/nfd/node-feature-discovery:v0.16.6"
          imagePullPolicy: IfNotPresent
          livenessProbe:
            grpc:
              port: 8082
            initialDelaySeconds: 10
          readinessProbe:
            failureThreshold: 10
            grpc:
              port: 8082
            initialDelaySeconds: 5
          ports:
          - containerPort: 8080
            name: grpc
          - containerPort: 8081
            name: metrics
          - containerPort: 8082
            name: health
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          command:
            - "nfd-master"
          resources:
            limits:
              memory: 4Gi
            requests:
              cpu: 100m
              memory: 128Mi
          args:
            ## By default, disable crd controller for other than the default instances
            - "-crd-controller=true"
            # Go over featureGates and add the feature-gate flag
            - "-feature-gates=NodeFeatureAPI=true"
            - "-feature-gates=NodeFeatureGroupAPI=false"
            - "-metrics=8081"
            - "-grpc-health=8082"
          volumeMounts:
            - name: nfd-master-conf
              mountPath: "/etc/kubernetes/node-feature-discovery"
              readOnly: true
      volumes:
        - name: nfd-master-conf
          configMap:
            name: gpu-operator-node-feature-discovery-master-conf
            items:
              - key: nfd-master.conf
                path: nfd-master.conf
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: node-role.kubernetes.io/master
                operator: In
                values:
                - ""
            weight: 1
          - preference:
              matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: In
                values:
                - ""
            weight: 1
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Equal
          value: ""
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Equal
          value: ""
---
# Source: gpu-operator/charts/node-feature-discovery/templates/nfd-gc.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpu-operator-node-feature-discovery-gc
  namespace: default
  labels:
    helm.sh/chart: node-feature-discovery-0.16.6
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.16.6"
    app.kubernetes.io/managed-by: Helm
    role: gc
spec:
  replicas: 1
  revisionHistoryLimit: 
  selector:
    matchLabels:
      app.kubernetes.io/name: node-feature-discovery
      app.kubernetes.io/instance: gpu-operator
      role: gc
  template:
    metadata:
      labels:
        app.kubernetes.io/name: node-feature-discovery
        app.kubernetes.io/instance: gpu-operator
        role: gc
    spec:
      serviceAccountName: node-feature-discovery
      dnsPolicy: ClusterFirstWithHostNet
      priorityClassName: system-node-critical
      securityContext:
        {}
      hostNetwork: false
      containers:
      - name: gc
        image: "registry.k8s.io/nfd/node-feature-discovery:v0.16.6"
        imagePullPolicy: "IfNotPresent"
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        command:
          - "nfd-gc"
        args:
          - "-gc-interval=1h"
        resources:
            limits:
              memory: 1Gi
            requests:
              cpu: 10m
              memory: 128Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: [ "ALL" ]
          readOnlyRootFilesystem: true
          runAsNonRoot: true
        ports:
          - name: metrics
            containerPort: 8081
---
# Source: gpu-operator/templates/operator.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpu-operator
  labels:
    app.kubernetes.io/name: gpu-operator
    helm.sh/chart: gpu-operator-v24.9.2
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v24.9.2"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: "gpu-operator"
    nvidia.com/gpu-driver-upgrade-drain.skip: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: "gpu-operator"
      app: "gpu-operator"
  template:
    metadata:
      labels:
        app.kubernetes.io/name: gpu-operator
        helm.sh/chart: gpu-operator-v24.9.2
        app.kubernetes.io/instance: gpu-operator
        app.kubernetes.io/version: "v24.9.2"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: "gpu-operator"
        app: "gpu-operator"
        nvidia.com/gpu-driver-upgrade-drain.skip: "true"
      annotations:
        openshift.io/scc: restricted-readonly
    spec:
      serviceAccountName: gpu-operator
      priorityClassName: system-node-critical
      containers:
      - name: gpu-operator
        image: nvcr.io/nvidia/gpu-operator:v24.9.2
        imagePullPolicy: IfNotPresent
        command: ["gpu-operator"]
        args:
        - --leader-elect
        - --zap-time-encoding=epoch
        - --zap-log-level=info
        env:
        - name: WATCH_NAMESPACE
          value: ""
        - name: OPERATOR_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: "DRIVER_MANAGER_IMAGE"
          value: "nvcr.io/nvidia/cloud-native/k8s-driver-manager:v0.7.0"
        volumeMounts:
          - name: host-os-release
            mountPath: "/host-etc/os-release"
            readOnly: true
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8081
          initialDelaySeconds: 15
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8081
          initialDelaySeconds: 5
          periodSeconds: 10
        resources:
          limits:
            cpu: 500m
            memory: 350Mi
          requests:
            cpu: 200m
            memory: 100Mi
        ports:
          - name: metrics
            containerPort: 8080
      volumes:
        - name: host-os-release
          hostPath:
            path: "/etc/os-release"
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: node-role.kubernetes.io/master
                operator: In
                values:
                - ""
            weight: 1
          - preference:
              matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: In
                values:
                - ""
            weight: 1
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Equal
          value: ""
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Equal
          value: ""
---
# Source: gpu-operator/templates/clusterpolicy.yaml
apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: cluster-policy
  labels:
    app.kubernetes.io/name: gpu-operator
    helm.sh/chart: gpu-operator-v24.9.2
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v24.9.2"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: "gpu-operator"
spec:
  hostPaths:
    rootFS: /
    driverInstallDir: /run/nvidia/driver
  operator:
    defaultRuntime: docker
    runtimeClass: nvidia
    initContainer:
      repository: nvcr.io/nvidia
      image: cuda
      version: "12.6.3-base-ubi9"
      imagePullPolicy: IfNotPresent
  daemonsets:
    labels:
      helm.sh/chart: gpu-operator-v24.9.2
      app.kubernetes.io/managed-by: gpu-operator
    tolerations: 
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Exists
    priorityClassName: system-node-critical
    updateStrategy: RollingUpdate
    rollingUpdate:
      maxUnavailable: "1"
  validator:
    repository: nvcr.io/nvidia/cloud-native
    image: gpu-operator-validator
    version: "v24.9.2"
    imagePullPolicy: IfNotPresent
    plugin:
      env: 
        - name: WITH_WORKLOAD
          value: "false"

  mig:
    strategy: single
  psa:
    enabled: false
  cdi:
    enabled: false
    default: false
  driver:
    enabled: true
    useNvidiaDriverCRD: false
    useOpenKernelModules: false
    usePrecompiled: false
    repository: nvcr.io/nvidia
    image: driver
    version: "550.144.03"
    imagePullPolicy: IfNotPresent
    startupProbe: 
      failureThreshold: 120
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 60
    rdma:
      enabled: false
      useHostMofed: false
    manager:
      repository: nvcr.io/nvidia/cloud-native
      image: k8s-driver-manager
      version: "v0.7.0"
      imagePullPolicy: IfNotPresent
      env: 
        - name: ENABLE_GPU_POD_EVICTION
          value: "true"
        - name: ENABLE_AUTO_DRAIN
          value: "false"
        - name: DRAIN_USE_FORCE
          value: "false"
        - name: DRAIN_POD_SELECTOR_LABEL
          value: ""
        - name: DRAIN_TIMEOUT_SECONDS
          value: 0s
        - name: DRAIN_DELETE_EMPTYDIR_DATA
          value: "false"
    repoConfig: 
      configMapName: ""
    certConfig: 
      name: ""
    licensingConfig: 
      configMapName: ""
      nlsEnabled: true
    virtualTopology: 
      config: ""
    kernelModuleConfig: 
      name: ""
    upgradePolicy:
      autoUpgrade: true
      maxParallelUpgrades: 1
      maxUnavailable : 25%
      waitForCompletion:
        timeoutSeconds: 0
      podDeletion:
        force: false
        timeoutSeconds: 300
        deleteEmptyDir: false
      drain:
        enable: false
        force: false
        timeoutSeconds: 300
        deleteEmptyDir: false
  vgpuManager:
    enabled: false
    image: vgpu-manager
    imagePullPolicy: IfNotPresent
    driverManager:
      repository: nvcr.io/nvidia/cloud-native
      image: k8s-driver-manager
      version: "v0.7.0"
      imagePullPolicy: IfNotPresent
      env: 
        - name: ENABLE_GPU_POD_EVICTION
          value: "false"
        - name: ENABLE_AUTO_DRAIN
          value: "false"
  kataManager:
    enabled: false
    config: 
      artifactsDir: /opt/nvidia-gpu-operator/artifacts/runtimeclasses
      runtimeClasses:
      - artifacts:
          pullSecret: ""
          url: nvcr.io/nvidia/cloud-native/kata-gpu-artifacts:ubuntu22.04-535.54.03
        name: kata-nvidia-gpu
        nodeSelector: {}
      - artifacts:
          pullSecret: ""
          url: nvcr.io/nvidia/cloud-native/kata-gpu-artifacts:ubuntu22.04-535.86.10-snp
        name: kata-nvidia-gpu-snp
        nodeSelector:
          nvidia.com/cc.capable: "true"
    repository: nvcr.io/nvidia/cloud-native
    image: k8s-kata-manager
    version: "v0.2.2"
    imagePullPolicy: IfNotPresent
  vfioManager:
    enabled: true
    repository: nvcr.io/nvidia
    image: cuda
    version: "12.6.3-base-ubi9"
    imagePullPolicy: IfNotPresent
    driverManager:
      repository: nvcr.io/nvidia/cloud-native
      image: k8s-driver-manager
      version: "v0.7.0"
      imagePullPolicy: IfNotPresent
      env: 
        - name: ENABLE_GPU_POD_EVICTION
          value: "false"
        - name: ENABLE_AUTO_DRAIN
          value: "false"
  vgpuDeviceManager:
    enabled: true
    repository: nvcr.io/nvidia/cloud-native
    image: vgpu-device-manager
    version: "v0.2.8"
    imagePullPolicy: IfNotPresent
    config: 
      default: default
      name: ""
  ccManager:
    enabled: false
    defaultMode: "off"
    repository: nvcr.io/nvidia/cloud-native
    image: k8s-cc-manager
    version: "v0.1.1"
    imagePullPolicy: IfNotPresent
    env: 
      []
  toolkit:
    enabled: true
    repository: nvcr.io/nvidia/k8s
    image: container-toolkit
    version: "v1.17.4-ubuntu20.04"
    imagePullPolicy: IfNotPresent
    installDir: /usr/local/nvidia
  devicePlugin:
    enabled: true
    repository: nvcr.io/nvidia
    image: k8s-device-plugin
    version: "v0.17.0"
    imagePullPolicy: IfNotPresent
    env: 
      - name: PASS_DEVICE_SPECS
        value: "true"
      - name: FAIL_ON_INIT_ERROR
        value: "true"
      - name: DEVICE_LIST_STRATEGY
        value: envvar
      - name: DEVICE_ID_STRATEGY
        value: uuid
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      - name: NVIDIA_DRIVER_CAPABILITIES
        value: all
  dcgm:
    enabled: false
    repository: nvcr.io/nvidia/cloud-native
    image: dcgm
    version: "3.3.9-1-ubuntu22.04"
    imagePullPolicy: IfNotPresent
  dcgmExporter:
    enabled: true
    repository: nvcr.io/nvidia/k8s
    image: dcgm-exporter
    version: "3.3.9-3.6.1-ubuntu22.04"
    imagePullPolicy: IfNotPresent
    env: 
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_COLLECTORS
        value: /etc/dcgm-exporter/dcp-metrics-included.csv
    serviceMonitor: 
      additionalLabels: {}
      enabled: false
      honorLabels: false
      interval: 15s
      relabelings: []
  gfd:
    enabled: true
    repository: nvcr.io/nvidia
    image: k8s-device-plugin
    version: "v0.17.0"
    imagePullPolicy: IfNotPresent
    env: 
      - name: GFD_SLEEP_INTERVAL
        value: 60s
      - name: GFD_FAIL_ON_INIT_ERROR
        value: "true"
  migManager:
    enabled: true
    repository: nvcr.io/nvidia/cloud-native
    image: k8s-mig-manager
    version: "v0.10.0-ubuntu20.04"
    imagePullPolicy: IfNotPresent
    env: 
      - name: WITH_REBOOT
        value: "false"
    config:
      name: 
      default: all-disabled
    gpuClientsConfig: 
      name: ""
  nodeStatusExporter:
    enabled: false
    repository: nvcr.io/nvidia/cloud-native
    image: gpu-operator-validator
    version: "v24.9.2"
    imagePullPolicy: IfNotPresent
  gdrcopy:
    enabled: false
    repository: nvcr.io/nvidia/cloud-native
    image: gdrdrv
    version: "v2.4.1-2"
    imagePullPolicy: IfNotPresent
  sandboxWorkloads:
    enabled: false
    defaultWorkload: container
  sandboxDevicePlugin:
    enabled: true
    repository: nvcr.io/nvidia
    image: kubevirt-gpu-device-plugin
    version: "v1.2.10"
    imagePullPolicy: IfNotPresent
---
# Source: gpu-operator/charts/node-feature-discovery/templates/post-delete-job.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: gpu-operator-node-feature-discovery-prune
  namespace: default
  labels:
    helm.sh/chart: node-feature-discovery-0.16.6
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.16.6"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": post-delete
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
---
# Source: gpu-operator/templates/upgrade_crd.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: gpu-operator-upgrade-crd-hook-sa
  annotations:
    helm.sh/hook: pre-upgrade
    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation
    helm.sh/hook-weight: "0"
---
# Source: gpu-operator/charts/node-feature-discovery/templates/post-delete-job.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: gpu-operator-node-feature-discovery-prune
  labels:
    helm.sh/chart: node-feature-discovery-0.16.6
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.16.6"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": post-delete
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
rules:
- apiGroups:
  - ""
  resources:
  - nodes
  - nodes/status
  verbs:
  - get
  - patch
  - update
  - list
---
# Source: gpu-operator/templates/upgrade_crd.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: gpu-operator-upgrade-crd-hook-role
  annotations:
    helm.sh/hook: pre-upgrade
    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation
    helm.sh/hook-weight: "0"
rules:
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - create
      - get
      - list
      - watch
      - patch
      - update
---
# Source: gpu-operator/charts/node-feature-discovery/templates/post-delete-job.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: gpu-operator-node-feature-discovery-prune
  labels:
    helm.sh/chart: node-feature-discovery-0.16.6
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.16.6"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": post-delete
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: gpu-operator-node-feature-discovery-prune
subjects:
- kind: ServiceAccount
  name: gpu-operator-node-feature-discovery-prune
  namespace: default
---
# Source: gpu-operator/templates/upgrade_crd.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: gpu-operator-upgrade-crd-hook-binding
  annotations:
    helm.sh/hook: pre-upgrade
    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation
    helm.sh/hook-weight: "0"
subjects:
  - kind: ServiceAccount
    name: gpu-operator-upgrade-crd-hook-sa
    namespace: default
roleRef:
  kind: ClusterRole
  name: gpu-operator-upgrade-crd-hook-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: gpu-operator/charts/node-feature-discovery/templates/post-delete-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name:  gpu-operator-node-feature-discovery-prune
  namespace: default
  labels:
    helm.sh/chart: node-feature-discovery-0.16.6
    app.kubernetes.io/name: node-feature-discovery
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v0.16.6"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": post-delete
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
spec:
  template:
    metadata:
      labels:
        helm.sh/chart: node-feature-discovery-0.16.6
        app.kubernetes.io/name: node-feature-discovery
        app.kubernetes.io/instance: gpu-operator
        app.kubernetes.io/version: "v0.16.6"
        app.kubernetes.io/managed-by: Helm
        role: prune
    spec:
      serviceAccountName: gpu-operator-node-feature-discovery-prune
      containers:
        - name: nfd-master
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
          image: "registry.k8s.io/nfd/node-feature-discovery:v0.16.6"
          imagePullPolicy: IfNotPresent
          command:
            - "nfd-master"
          args:
            - "-prune"
      restartPolicy: Never
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: node-role.kubernetes.io/master
                operator: In
                values:
                - ""
            weight: 1
          - preference:
              matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: In
                values:
                - ""
            weight: 1
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Equal
          value: ""
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Equal
          value: ""
---
# Source: gpu-operator/templates/upgrade_crd.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: gpu-operator-upgrade-crd
  namespace: default
  annotations:
    "helm.sh/hook": pre-upgrade
    "helm.sh/hook-weight": "1"
    "helm.sh/hook-delete-policy": hook-succeeded,before-hook-creation
  labels:
    app.kubernetes.io/name: gpu-operator
    helm.sh/chart: gpu-operator-v24.9.2
    app.kubernetes.io/instance: gpu-operator
    app.kubernetes.io/version: "v24.9.2"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: "gpu-operator"
spec:
  template:
    metadata:
      name: gpu-operator-upgrade-crd
      labels:
        app.kubernetes.io/name: gpu-operator
        helm.sh/chart: gpu-operator-v24.9.2
        app.kubernetes.io/instance: gpu-operator
        app.kubernetes.io/version: "v24.9.2"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: "gpu-operator"
    spec:
      serviceAccountName: gpu-operator-upgrade-crd-hook-sa
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Equal
          value: ""
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Equal
          value: ""
      containers:
        - name: upgrade-crd
          image: nvcr.io/nvidia/gpu-operator:v24.9.2
          imagePullPolicy: IfNotPresent
          command:
          - /bin/sh
          - -c
          - >
            kubectl apply -f /opt/gpu-operator/nvidia.com_clusterpolicies.yaml;
            kubectl apply -f /opt/gpu-operator/nvidia.com_nvidiadrivers.yaml;
            kubectl apply -f /opt/gpu-operator/nfd-api-crds.yaml;
      restartPolicy: OnFailure
---
# ----- END NVIDIA GPU OPERATOR -----
# ----- START DCGM SERVICEMONITOR -----
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: nvidia-dcgm-exporter
  namespace: devzero-zxporter
  labels:
    release: prometheus
spec:
  selector:
    matchLabels:
      app: nvidia-dcgm-exporter
  namespaceSelector:
    matchNames:
      - devzero-zxporter
  endpoints:
    - port: gpu-metrics
      interval: 15s
---
# ----- END DCGM SERVICEMONITOR -----
# ----- START METRICS SERVER -----
# Note: Comment out this section if metrics-server is already installed
---
# Source: metrics-server/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: metrics-server
  namespace: kube-system
  labels:
    helm.sh/chart: metrics-server-3.12.0
    app.kubernetes.io/name: metrics-server
    app.kubernetes.io/instance: metrics-server
    app.kubernetes.io/version: "0.7.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: metrics-server/templates/clusterrole-aggregated-reader.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: system:metrics-server-aggregated-reader
  labels:
    helm.sh/chart: metrics-server-3.12.0
    app.kubernetes.io/name: metrics-server
    app.kubernetes.io/instance: metrics-server
    app.kubernetes.io/version: "0.7.0"
    app.kubernetes.io/managed-by: Helm
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
    rbac.authorization.k8s.io/aggregate-to-view: "true"
rules:
  - apiGroups:
      - metrics.k8s.io
    resources:
      - pods
      - nodes
    verbs:
      - get
      - list
      - watch
---
# Source: metrics-server/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: system:metrics-server
  labels:
    helm.sh/chart: metrics-server-3.12.0
    app.kubernetes.io/name: metrics-server
    app.kubernetes.io/instance: metrics-server
    app.kubernetes.io/version: "0.7.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
    - ""
    resources:
    - nodes/metrics
    verbs:
    - get
  - apiGroups:
    - ""
    resources:
      - pods
      - nodes
      - namespaces
      - configmaps
    verbs:
      - get
      - list
      - watch
---
# Source: metrics-server/templates/clusterrolebinding-auth-delegator.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: metrics-server:system:auth-delegator
  labels:
    helm.sh/chart: metrics-server-3.12.0
    app.kubernetes.io/name: metrics-server
    app.kubernetes.io/instance: metrics-server
    app.kubernetes.io/version: "0.7.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
  - kind: ServiceAccount
    name: metrics-server
    namespace: kube-system
---
# Source: metrics-server/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:metrics-server
  labels:
    helm.sh/chart: metrics-server-3.12.0
    app.kubernetes.io/name: metrics-server
    app.kubernetes.io/instance: metrics-server
    app.kubernetes.io/version: "0.7.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:metrics-server
subjects:
  - kind: ServiceAccount
    name: metrics-server
    namespace: kube-system
---
# Source: metrics-server/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: metrics-server-auth-reader
  namespace: kube-system
  labels:
    helm.sh/chart: metrics-server-3.12.0
    app.kubernetes.io/name: metrics-server
    app.kubernetes.io/instance: metrics-server
    app.kubernetes.io/version: "0.7.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: extension-apiserver-authentication-reader
subjects:
  - kind: ServiceAccount
    name: metrics-server
    namespace: kube-system
---
# Source: metrics-server/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: metrics-server
  namespace: kube-system
  labels:
    helm.sh/chart: metrics-server-3.12.0
    app.kubernetes.io/name: metrics-server
    app.kubernetes.io/instance: metrics-server
    app.kubernetes.io/version: "0.7.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
  selector:
    app.kubernetes.io/name: metrics-server
    app.kubernetes.io/instance: metrics-server
---
# Source: metrics-server/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics-server
  namespace: kube-system
  labels:
    helm.sh/chart: metrics-server-3.12.0
    app.kubernetes.io/name: metrics-server
    app.kubernetes.io/instance: metrics-server
    app.kubernetes.io/version: "0.7.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: metrics-server
      app.kubernetes.io/instance: metrics-server
  template:
    metadata:
      labels:
        app.kubernetes.io/name: metrics-server
        app.kubernetes.io/instance: metrics-server
    spec:
      schedulerName: 
      serviceAccountName: metrics-server
      priorityClassName: "system-cluster-critical"
      containers:
        - name: metrics-server
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
            seccompProfile:
              type: RuntimeDefault
          image: registry.k8s.io/metrics-server/metrics-server:v0.7.0
          imagePullPolicy: IfNotPresent
          args:
            - --secure-port=10250
            - --cert-dir=/tmp
            - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
            - --kubelet-use-node-status-port
            - --metric-resolution=15s
            - --kubelet-insecure-tls
          ports:
          - name: https
            protocol: TCP
            containerPort: 10250
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            initialDelaySeconds: 0
            periodSeconds: 10
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            initialDelaySeconds: 20
            periodSeconds: 10
          volumeMounts:
            - name: tmp
              mountPath: /tmp
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
      volumes:
        - name: tmp
          emptyDir: {}
---
# Source: metrics-server/templates/apiservice.yaml
apiVersion: apiregistration.k8s.io/v1
kind: APIService
metadata:
  name: v1beta1.metrics.k8s.io
  labels:
    helm.sh/chart: metrics-server-3.12.0
    app.kubernetes.io/name: metrics-server
    app.kubernetes.io/instance: metrics-server
    app.kubernetes.io/version: "0.7.0"
    app.kubernetes.io/managed-by: Helm
spec:
  group: metrics.k8s.io
  groupPriorityMinimum: 100
  insecureSkipTLSVerify: true
  service:
    name: metrics-server
    namespace: kube-system
    port: 443
  version: v1beta1
  versionPriority: 100
# ----- END METRICS SERVER -----
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/name: devzero-zxporter
    control-plane: controller-manager
  name: devzero-zxporter
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/name: devzero-zxporter
  name: devzero-zxporter-controller-manager
  namespace: devzero-zxporter
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/name: devzero-zxporter
  name: devzero-zxporter-leader-election-role
  namespace: devzero-zxporter
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/name: devzero-zxporter
  name: devzero-zxporter-collectionpolicy-editor-role
rules:
- apiGroups:
  - devzero.io
  resources:
  - collectionpolicies
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - devzero.io
  resources:
  - collectionpolicies/status
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/name: devzero-zxporter
  name: devzero-zxporter-collectionpolicy-viewer-role
rules:
- apiGroups:
  - devzero.io
  resources:
  - collectionpolicies
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - devzero.io
  resources:
  - collectionpolicies/status
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: devzero-zxporter-manager-role
rules:
- apiGroups:
  - apps
  resources:
  - daemonsets
  - deployments
  - replicasets
  - statefulsets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - argoproj.io
  resources:
  - rollouts
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - autoscaling
  resources:
  - horizontalpodautoscalers
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - autoscaling.k8s.io
  resources:
  - verticalpodautoscalers
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - batch
  resources:
  - cronjobs
  - jobs
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - endpoints
  - events
  - limitranges
  - namespaces
  - nodes
  - persistentvolumeclaims
  - persistentvolumes
  - pods
  - replicationcontrollers
  - resourcequotas
  - serviceaccounts
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes/status
  - pods/status
  verbs:
  - get
- apiGroups:
  - datadoghq.com
  resources:
  - extendeddaemonsetreplicasets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - devzero.io
  resources:
  - collectionpolicies
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - devzero.io
  resources:
  - collectionpolicies/finalizers
  verbs:
  - update
- apiGroups:
  - devzero.io
  resources:
  - collectionpolicies/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - karpenter.k8s.aws
  resources:
  - awsnodetemplates
  - ec2nodeclasses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - karpenter.sh
  resources:
  - machines
  - nodeclaims
  - nodepools
  - provisioners
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - metrics.k8s.io
  resources:
  - nodes
  - pods
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingressclasses
  - ingresses
  - networkpolicies
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - clusterrolebindings
  - clusterroles
  - rolebindings
  - roles
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - storage.k8s.io
  resources:
  - csinodes
  - storageclasses
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: devzero-zxporter-metrics-auth-role
rules:
- apiGroups:
  - authentication.k8s.io
  resources:
  - tokenreviews
  verbs:
  - create
- apiGroups:
  - authorization.k8s.io
  resources:
  - subjectaccessreviews
  verbs:
  - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: devzero-zxporter-metrics-reader
rules:
- nonResourceURLs:
  - /metrics
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/name: devzero-zxporter
  name: devzero-zxporter-leader-election-rolebinding
  namespace: devzero-zxporter
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: devzero-zxporter-leader-election-role
subjects:
- kind: ServiceAccount
  name: devzero-zxporter-controller-manager
  namespace: devzero-zxporter
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/name: devzero-zxporter
  name: devzero-zxporter-manager-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: devzero-zxporter-manager-role
subjects:
- kind: ServiceAccount
  name: devzero-zxporter-controller-manager
  namespace: devzero-zxporter
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: devzero-zxporter-metrics-auth-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: devzero-zxporter-metrics-auth-role
subjects:
- kind: ServiceAccount
  name: devzero-zxporter-controller-manager
  namespace: devzero-zxporter
---
apiVersion: v1
data:
  BUFFER_SIZE: ""
  CLUSTER_TOKEN: '{{ .cluster_token }}'
  COLLECTION_FREQUENCY: ""
  DAKR_URL: ""
  DISABLE_NETWORK_IO_METRICS: ""
  DISABLED_COLLECTORS: ""
  EXCLUDED_CLUSTERROLEBINDINGS: ""
  EXCLUDED_CLUSTERROLES: ""
  EXCLUDED_CRDGROUPS: ""
  EXCLUDED_CRDS: ""
  EXCLUDED_CRONJOBS: ""
  EXCLUDED_DAEMONSETS: ""
  EXCLUDED_DEPLOYMENTS: ""
  EXCLUDED_ENDPOINTS: ""
  EXCLUDED_EVENTS: ""
  EXCLUDED_HPAS: ""
  EXCLUDED_INGRESSCLASSES: ""
  EXCLUDED_INGRESSES: ""
  EXCLUDED_JOBS: ""
  EXCLUDED_LIMITRANGES: ""
  EXCLUDED_NAMESPACES: ""
  EXCLUDED_NETWORKPOLICIES: ""
  EXCLUDED_NODES: ""
  EXCLUDED_PDBS: ""
  EXCLUDED_PODS: ""
  EXCLUDED_PSPS: ""
  EXCLUDED_PVCS: ""
  EXCLUDED_PVS: ""
  EXCLUDED_REPLICATIONCONTROLLERS: ""
  EXCLUDED_RESOURCEQUOTAS: ""
  EXCLUDED_ROLEBINDINGS: ""
  EXCLUDED_ROLES: ""
  EXCLUDED_SERVICEACCOUNTS: ""
  EXCLUDED_SERVICES: ""
  EXCLUDED_STATEFULSETS: ""
  EXCLUDED_STORAGECLASSES: ""
  EXCLUDED_VPAS: ""
  K8S_PROVIDER: '{{ .k8s_provider }}'
  MASK_SECRET_DATA: ""
  NODE_METRICS_INTERVAL: ""
  PROMETHEUS_URL: http://prometheus-server.devzero-zxporter.svc.cluster.local:80
  TARGET_NAMESPACES: ""
  WATCHED_CRDS: ""
kind: ConfigMap
metadata:
  name: devzero-zxporter-env-config
  namespace: devzero-zxporter
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/name: devzero-zxporter
    control-plane: controller-manager
  name: devzero-zxporter-controller-manager-metrics-service
  namespace: devzero-zxporter
spec:
  ports:
  - name: https
    port: 8443
    protocol: TCP
    targetPort: 8443
  selector:
    control-plane: controller-manager
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/name: devzero-zxporter
    control-plane: controller-manager
  name: devzero-zxporter-controller-manager
  namespace: devzero-zxporter
spec:
  replicas: 1
  selector:
    matchLabels:
      control-plane: controller-manager
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: manager
      labels:
        control-plane: controller-manager
    spec:
      containers:
      - args:
        - --metrics-bind-address=:8443
        - --leader-elect
        - --health-probe-bind-address=:8081
        command:
        - /manager
        image: docker.io/remontada007/zxporter:v99
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8081
          initialDelaySeconds: 15
          periodSeconds: 20
        name: manager
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8081
          initialDelaySeconds: 5
          periodSeconds: 10
        resources:
          requests:
            cpu: 200m
            memory: 128Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
        volumeMounts:
        - mountPath: /etc/zxporter/config
          name: config-volume
          readOnly: true
      securityContext:
        runAsNonRoot: true
      serviceAccountName: devzero-zxporter-controller-manager
      terminationGracePeriodSeconds: 10
      volumes:
      - configMap:
          name: devzero-zxporter-env-config
        name: config-volume
