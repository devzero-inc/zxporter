name: AWS GPU Test

on:
  push:
    branches:
      - garvit/aws-gpu-test
  workflow_dispatch:

permissions:
  id-token: write
  contents: read

jobs:
  apply-terraform:
    name: Apply Terraform
    runs-on: ubuntu-latest

    outputs:
      job_identifier: ${{ steps.job-identifier.outputs.job_identifier }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Configure AWS Credential
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::484907513542:role/github-actions-oidc-role
          aws-region: us-east-1

      - name: Generate Unique Job Identifier
        id: job-identifier
        shell: bash
        run: |
          SHORT_SHA=$(git rev-parse --short HEAD)
          JOB_IDENTIFIER="gh-ci-ro-${SHORT_SHA}"
          echo "JOB_IDENTIFIER=${JOB_IDENTIFIER}" >> $GITHUB_ENV
          echo "job_identifier=${JOB_IDENTIFIER}" >> $GITHUB_OUTPUT

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.5.7

      - name: Apply Terraform
        working-directory: terraform/aws
        run: |
          cat <<EOF > backend_override.tf
          terraform {
            backend "s3" {
                bucket         	   = "zxporter-tf-state"
                key              	 = "${JOB_IDENTIFIER}/terraform.tfstate"
                region         	   = "us-east-1"
            }
          }
          EOF
          terraform init
          terraform apply -auto-approve -var="cluster_name=$JOB_IDENTIFIER"

  install-and-validate:
    name: Install and Validate GPU Resources and ZXPorter
    runs-on: ubuntu-latest
    needs: apply-terraform 

    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::484907513542:role/github-actions-oidc-role
          aws-region: us-east-1

      - name: Configure Kubernetes Access
        run: |
          aws eks update-kubeconfig --region us-east-1 --name ${{ needs.apply-terraform.outputs.job_identifier }}

      - name: Check GPU Availability
        id: gpu_check
        run: |
          echo "Checking GPU resources on nodes..."
          if kubectl describe nodes | grep -q "nvidia.com/gpu"; then
            echo "GPU resources are available on the nodes."
            echo "GPU_CHECK=true" >> $GITHUB_ENV
          else
            echo "GPU check failed"
            echo "GPU_CHECK=false" >> $GITHUB_ENV
          fi

      - name: Install GPU Operator (if needed)
        if: env.GPU_CHECK == 'false'
        run: |
          echo "GPU resources not found, installing GPU Operator..."
          kubectl create ns gpu-operator
          kubectl label ns gpu-operator pod-security.kubernetes.io/enforce=privileged --overwrite
          kubectl get nodes -o json | jq '.items[].metadata.labels | keys | any(startswith("feature.node.kubernetes.io"))' || true
          helm repo add nvidia https://helm.ngc.nvidia.com/nvidia && \
          helm repo update
          helm install --wait --generate-name -n gpu-operator --create-namespace nvidia/gpu-operator --version=v25.3.0

      - name: Check GPU Availability After Installing GPU Operator
        if: env.GPU_CHECK == 'false'
        run: |
          echo "Re-checking GPU resources on nodes after GPU Operator installation..."
          if kubectl describe nodes | grep -q "nvidia.com/gpu"; then
            echo "GPU resources are available on the nodes."
          else
            echo "GPU check failed after GPU Operator installation"
            exit 1
          fi

      - name: Check DCGM DaemonSet
        id: dcgm_check
        run: |
          echo "Checking if DCGM DaemonSet is installed..."
          if kubectl get daemonset -A | grep -q dcgm; then
            echo "DCGM DaemonSet is already installed."
            echo "DCGM_CHECK=true" >> $GITHUB_ENV 
          else
            echo "DCGM DaemonSet not found."
            echo "DCGM_CHECK=false" >> $GITHUB_ENV
          fi

      - name: Install DCGM Exporter (if needed)
        if: env.DCGM_CHECK == 'false'
        run: |
          echo "Installing DCGM Exporter..."
          kubectl create ns devzero-zxporter
          curl https://raw.githubusercontent.com/devzero-inc/zxporter/refs/heads/main/dcgm-installers/eks.yml | kubectl apply -f -

      - name: Check DCGM DaemonSet After Installing DCGM Exporter
        if: env.DCGM_CHECK == 'false'
        run: |
          echo "Re-checking DCGM pods after DCGM Exporter installation..."
          if kubectl get daemonset -A | grep -q dcgm; then
            echo "DCGM DaemonSet is running."
          else
            echo "DCGM DaemonSet not running after installation"
            exit 1
          fi
          
      - name: Verify DCGM Pods and Prometheus Annotations
        run: |
          kubectl get pods -n gpu-operator -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}' | grep dcgm-exporter | xargs -r -I {} kubectl wait --for=condition=Ready pod {} -n gpu-operator --timeout=300s
          echo "Verifying DCGM pods and Prometheus annotations..."
          kubectl get pods -A | grep dcgm-exporter | awk '
          BEGIN { all_running = 1; pod_count = 0 }
          {
              pod_count++
              status = $4
              printf "Pod: %s/%s - Status: %s\n", $1, $2, status
              if (status != "Running") all_running = 0
          }
          END {
              printf "\nTotal Pods: %d\n", pod_count
              printf "All Running: %s\n", (all_running ? "true" : "false")
          }'
          kubectl get pods -A -o json | jq -r '.items[] | select(.metadata.name | contains("dcgm-exporter")) | "\(.metadata.namespace) \(.metadata.name)"' | while read namespace pod; do kubectl annotate pod $pod -n $namespace prometheus.io/scrape=true --overwrite; done

      - name: Install and Verify DeepSeek Workload
        run: |
          kubectl create ns deepseek
          kubectl apply -f https://gist.githubusercontent.com/Tzvonimir/a168dcc1515d3bf89254c34010e16d37/raw/4b154383f4e254c9490d4815e85aa5f574eb26eb/install-test-deepseek.yaml    
          
          kubectl wait --for=condition=ready pod -n deepseek --all --timeout=600s
          pod_status=$(kubectl get pods -n deepseek --field-selector=status.phase!=Running -o jsonpath='{.items[*].status.phase}')
          
          if [[ -n "$pod_status" ]]; then
            echo "Pods are not in Running state. Failing the pipeline."
            exit 1
          else
            echo "All pods are running successfully."
          fi

      - name: Install ZXPorter
        run: |
          curl -XPOST -H 'Authorization: Bearer dzu-bdef3HBkpAs-SfpVcHXH0VJFhVibZ2qRCL1IRdYRlIs=' \
            -H "X-Kube-Context-Name: $(kubectl config current-context)" \
            "https://api.devzero.io/backend/v0/dakr/installer-manifest?cluster-provider=aws" | \
            kubectl apply -f -
          
          echo "Waiting for ZXPorter pods to be ready..."
          kubectl wait --for=condition=Ready pod -l app.kubernetes.io/component=server -n devzero-zxporter --timeout=300s

      - name: Test ZXPorter with Prometheus
        run: |
          kubectl port-forward svc/prometheus-server 9090:80 -n devzero-zxporter &
          sleep 5
          result=$(curl -s "http://localhost:9090/api/v1/query?query=DCGM_FI_DEV_SM_CLOCK" | jq -r '.data.result')
          if [[ -z "$result" || "$result" == "null" ]]; then
            echo "DCGM_FI_DEV_SM_CLOCK metric not found!"
            exit 1
          fi
          echo "Metric found: $result"

  destroy-terraform:
    name: Destroy Terraform
    runs-on: ubuntu-latest
    if: always()
    needs:
      - apply-terraform
      - install-and-validate

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::484907513542:role/github-actions-oidc-role
          aws-region: us-east-1

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.5.7

      - name: Destroy Infrastructure
        working-directory: terraform/aws
        run: |
          cat <<EOF > backend_override.tf
          terraform {
            backend "s3" {
                bucket  = "zxporter-tf-state"
                key     = "${{ needs.apply-terraform.outputs.job_identifier }}/terraform.tfstate"
                region  = "us-east-1"
            }
          }
          EOF
          terraform init
          terraform destroy -auto-approve -var="cluster_name=${{ needs.apply-terraform.outputs.job_identifier }}"
