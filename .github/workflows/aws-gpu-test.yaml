name: AWS GPU Test

on:
  push:
    branches:
      - garvit/aws-gpu-test
  workflow_dispatch:
    inputs:
      gpu_install_type:
        description: 'GPU installation type'
        required: false
        default: 'nvidia-device-plugin'
        type: choice
        options:
          - gpu-operator
          - nvidia-device-plugin
      dcgm_install_type:
        description: 'DCGM install type'
        required: false
        default: 'devzero-dcgm'
        type: choice
        options:
          - nvidia-dcgm
          - devzero-dcgm
      cluster_version:
        description: 'Kubernetes cluster version'
        required: false
        default: '1.30'
        type: choice
        options:
          - '1.26'
          - '1.27'
          - '1.28'
          - '1.29'
          - '1.30'
          - '1.31'
          - '1.32'
          - '1.33'
      karpenter_version:
        description: 'Karpenter Version'
        required: false
        default: '0.37.7'
        type: choice
        options:
          - 'no_karpenter'
          - '0.37.7'

permissions:
  id-token: write
  contents: read

jobs:
  apply-terraform:
    name: Apply Terraform
    runs-on: ubuntu-latest
    env:
      GPU_INSTALL_TYPE: ${{ github.event.inputs.gpu_install_type || 'nvidia-device-plugin' }}
      DCGM_INSTALL_TYPE: ${{ github.event.inputs.dcgm_install_type || 'devzero-dcgm' }}
      CLUSTER_VERSION: ${{ github.event.inputs.cluster_version || '1.30' }}

    outputs:
      job_identifier: ${{ steps.job-identifier.outputs.job_identifier }}

    steps:
      - name: Validate Inputs
        run: |
          echo "GPU_INSTALL_TYPE=${GPU_INSTALL_TYPE}"
          echo "DCGM_INSTALL_TYPE=${DCGM_INSTALL_TYPE}"

          if [[ "$GPU_INSTALL_TYPE" == "nvidia-device-plugin" && "$DCGM_INSTALL_TYPE" != "devzero-dcgm" ]]; then
            echo "Error: When GPU_INSTALL_TYPE is 'nvidia-device-plugin', DCGM_INSTALL_TYPE must be 'devzero-dcgm'."
            exit 1
          fi

      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Configure AWS Credential
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::484907513542:role/github-actions-oidc-role
          aws-region: us-east-1

      - name: Generate Unique Job Identifier
        id: job-identifier
        shell: bash
        run: |
          SHORT_SHA=$(git rev-parse --short HEAD)
          if [[ "$DCGM_INSTALL_TYPE" == "devzero-dcgm" ]]; then
            SUFFIX="dd"
          else
            SUFFIX="nd"
          fi
          JOB_IDENTIFIER="gh-ci-ro-${SHORT_SHA}-${SUFFIX}"
          echo "JOB_IDENTIFIER=${JOB_IDENTIFIER}" >> $GITHUB_ENV
          echo "job_identifier=${JOB_IDENTIFIER}" >> $GITHUB_OUTPUT

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Apply Terraform
        working-directory: terraform/aws
        run: |
          cat <<EOF > backend_override.tf
          terraform {
            backend "s3" {
                bucket         	   = "zxporter-tf-state"
                key              	 = "${JOB_IDENTIFIER}/terraform.tfstate"
                region         	   = "us-east-1"
            }
          }
          EOF
          terraform init
          terraform apply -auto-approve -var="cluster_name=$JOB_IDENTIFIER" -var='cluster_version=${{ env.CLUSTER_VERSION }}'

  install-and-validate:
    name: Install and Validate GPU Resources and ZXPorter
    runs-on: ubuntu-latest
    needs: apply-terraform
    env:
      GPU_INSTALL_TYPE: ${{ github.event.inputs.gpu_install_type || 'nvidia-device-plugin' }}
      DCGM_INSTALL_TYPE: ${{ github.event.inputs.dcgm_install_type || 'devzero-dcgm' }}
      Karpenter_VERSION: ${{ github.event.inputs.karpenter_version || '0.37.7' }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::484907513542:role/github-actions-oidc-role
          aws-region: us-east-1

      - name: Configure Kubernetes Access
        run: |
          aws eks update-kubeconfig --region us-east-1 --name ${{ needs.apply-terraform.outputs.job_identifier }}

      - name: Install Karpenter (if needed)
        if: env.Karpenter_VERSION != 'no_karpenter'
        run: |
          echo "Installing Karpenter..."
          AWS_ACCOUNT_ID="$(aws sts get-caller-identity --query Account --output text)"
          CLUSTER_ENDPOINT="$(aws eks describe-cluster --name "${{ needs.apply-terraform.outputs.job_identifier }}" --query "cluster.endpoint" --output text)"
          KARPENTER_IAM_ROLE_ARN="arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterControllerRole-${{ needs.apply-terraform.outputs.job_identifier }}"
          helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter \
            --version "${{ env.KARPENTER_VERSION}}" \
            --namespace kube-system \
            --create-namespace \
            --set settings.clusterName="${{ needs.apply-terraform.outputs.job_identifier }}" \
            --set settings.aws.clusterName="${{ needs.apply-terraform.outputs.job_identifier }}" \
            --set settings.aws.clusterEndpoint="${CLUSTER_ENDPOINT}" \
            --set settings.aws.defaultInstanceProfile="KarpenterNodeRole-${{ needs.apply-terraform.outputs.job_identifier }}" \
            --set settings.aws.interruptionQueueName="${{ needs.apply-terraform.outputs.job_identifier }}-karpenter-interruption" \
            --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="${KARPENTER_IAM_ROLE_ARN}" \
            --set controller.resources.requests.cpu="1" \
            --set controller.resources.requests.memory="1Gi" \
            --set controller.resources.limits.cpu="1" \
            --set controller.resources.limits.memory="1Gi" \
            --wait

      - name: Check GPU Availability
        id: gpu_check
        run: |
          echo "Checking GPU resources on nodes..."
          if kubectl describe nodes | grep -q "nvidia.com/gpu"; then
            echo "GPU resources are available on the nodes."
            echo "GPU_CHECK=true" >> $GITHUB_ENV
          else
            echo "GPU check failed"
            echo "GPU_CHECK=false" >> $GITHUB_ENV
          fi

      - name: Install GPU Operator (if needed)
        if: env.GPU_CHECK == 'false' && env.GPU_INSTALL_TYPE == 'gpu-operator'
        run: |
          echo "GPU resources not found, installing GPU Operator..."
          kubectl create ns gpu-operator
          kubectl label ns gpu-operator pod-security.kubernetes.io/enforce=privileged --overwrite
          kubectl get nodes -o json | jq '.items[].metadata.labels | keys | any(startswith("feature.node.kubernetes.io"))' || true
          helm repo add nvidia https://helm.ngc.nvidia.com/nvidia && \
          helm repo update
          INSTALL_CMD="helm install --wait --generate-name -n gpu-operator --create-namespace nvidia/gpu-operator --version=v25.3.0"
          if [[ "$DCGM_INSTALL_TYPE" == "devzero-dcgm" ]]; then
            INSTALL_CMD="$INSTALL_CMD --set dcgmExporter.enabled=false"
          fi
          echo "Running: $INSTALL_CMD"
          $INSTALL_CMD

      - name: Install Nvidia Device Plugin
        if: env.GPU_INSTALL_TYPE == 'nvidia-device-plugin' && env.GPU_CHECK == 'false'
        run: |
          echo "Installing Nvidia Device Plugin..."
          kubectl get nodes -l node_type=gpu -o jsonpath='{.items[*].metadata.name}' | xargs -I {} kubectl label node {} nvidia.com/gpu=true nvidia.com/mps.capable=true nvidia.com/gpu.present=true --overwrite
          kubectl create ns nvidia-device-plugin
          kubectl apply -f nvidia-device-plugin-prereq
          helm repo add nvdp https://nvidia.github.io/k8s-device-plugin
          helm repo update
          helm upgrade -i nvdp nvdp/nvidia-device-plugin \
            --namespace nvidia-device-plugin \
            --version 0.17.1

      - name: Check GPU Availability After Installing GPU Operator
        if: env.GPU_CHECK == 'false'
        run: |
          echo "Re-checking GPU resources on nodes after GPU Operator installation..."
          if kubectl describe nodes | grep -q "nvidia.com/gpu"; then
            echo "GPU resources are available on the nodes."
          else
            echo "GPU check failed after GPU Operator installation"
            exit 1
          fi

      - name: Check Nvidia DCGM DaemonSet
        id: dcgm_check
        if: ${{ env.DCGM_INSTALL_TYPE == 'nvidia-dcgm' }}
        run: |
          echo "Checking if DCGM DaemonSet is installed..."
          if kubectl get daemonset -A | grep -q dcgm; then
            echo "Nvidia DCGM found, proceeding with validation."
          else
            echo "Nvidia DCGM not found."
            exit 1
          fi

      - name: Install DevZero DCGM
        if: ${{ env.DCGM_INSTALL_TYPE == 'devzero-dcgm' }}
        run: |
          echo "Installing DCGM Exporter..."
          kubectl create ns devzero-zxporter
          curl https://raw.githubusercontent.com/devzero-inc/zxporter/refs/heads/main/dcgm-installers/eks.yml | kubectl apply -f -

      - name: Check DCGM DaemonSet After Installing DCGM Exporter
        if: ${{ env.DCGM_INSTALL_TYPE == 'devzero-dcgm' }}
        run: |
          echo "Re-checking DCGM pods after DCGM Exporter installation..."
          if kubectl get daemonset -A | grep -q dcgm; then
            echo "DCGM DaemonSet is running."
          else
            echo "DCGM DaemonSet not running after installation"
            exit 1
          fi
          
      - name: Verify DCGM Pods and Prometheus Annotations
        run: |
          NAMESPACE="devzero-zxporter"
          if [[ "$DCGM_INSTALL_TYPE" == "nvidia-dcgm" ]]; then
            NAMESPACE="gpu-operator"
          fi
          kubectl get pods -n $NAMESPACE -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}' | grep dcgm-exporter | xargs -r -I {} kubectl wait --for=condition=Ready pod {} -n $NAMESPACE --timeout=300s
          echo "Verifying DCGM pods and Prometheus annotations..."
          kubectl get pods -A | grep dcgm-exporter | awk '
          BEGIN { all_running = 1; pod_count = 0 }
          {
              pod_count++
              status = $4
              printf "Pod: %s/%s - Status: %s\n", $1, $2, status
              if (status != "Running") all_running = 0
          }
          END {
              printf "\nTotal Pods: %d\n", pod_count
              printf "All Running: %s\n", (all_running ? "true" : "false")
          }'
          kubectl get pods -A -o json | jq -r '.items[] | select(.metadata.name | contains("dcgm-exporter")) | "\(.metadata.namespace) \(.metadata.name)"' | while read namespace pod; do kubectl annotate pod $pod -n $namespace prometheus.io/scrape=true --overwrite; done

      - name: Install and Verify DeepSeek Workload
        run: |
          kubectl create ns deepseek
          kubectl apply -f https://gist.githubusercontent.com/Tzvonimir/a168dcc1515d3bf89254c34010e16d37/raw/4b154383f4e254c9490d4815e85aa5f574eb26eb/install-test-deepseek.yaml    
          
          kubectl wait --for=condition=ready pod -n deepseek --all --timeout=600s
          pod_status=$(kubectl get pods -n deepseek --field-selector=status.phase!=Running -o jsonpath='{.items[*].status.phase}')
          
          if [[ -n "$pod_status" ]]; then
            echo "Pods are not in Running state. Failing the pipeline."
            exit 1
          else
            echo "All pods are running successfully."
          fi

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.22'
          cache: true

      - name: Install ZXPorter
        run: |
          ZXPORTER_IMG="ttl.sh/$(uuidgen):2h"
          echo "Building and pushing zxporter image: ${ZXPORTER_IMG}"
          make docker-build docker-push IMG=${ZXPORTER_IMG}
          make deploy IMG=${ZXPORTER_IMG}
          
          echo "Waiting for ZXPorter pods to be ready..."
          kubectl wait --for=condition=Ready pod -l app.kubernetes.io/component=server -n devzero-zxporter --timeout=300s

      - name: Test ZXPorter with Prometheus
        run: |
          kubectl port-forward svc/prometheus-dz-prometheus-server 9090:80 -n devzero-zxporter > pf.log 2>&1 &
          PF_PID=$!
          sleep 5
          MAX_RETRIES=6
          for i in $(seq 1 $MAX_RETRIES); do
            if curl -s "http://localhost:9090/-/ready" >/dev/null; then
              echo "Prometheus port-forward is ready."
              break
            fi
            echo "[$i/$MAX_RETRIES] Waiting for Prometheus to become ready..."
            sleep 5
          done

          result=$(curl -s "http://localhost:9090/api/v1/query?query=DCGM_FI_DEV_SM_CLOCK" | jq -r '.data.result')
          kill $PF_PID || true

          echo "Metric found: $result"
          if [[ -z "$result" || "$result" == [] ]]; then
            echo "❌ DCGM_FI_DEV_SM_CLOCK metric not found!"
            echo "Port-forward log:"
            cat pf.log
            exit 1
          fi

      - name: Test Karpenter
        if: inputs.karpenter_version != 'no_karpenter'
        run: |
          echo "Verifying Karpenter installation..."
          kubectl port-forward -n kube-system service/karpenter 8000:8000 > /dev/null 2>&1 &
          PF_PID=$!
          
          # Allow port-forward to establish
          sleep 5
          
          MAX_RETRIES=6
          HEALTH=""
          
          for i in $(seq 1 $MAX_RETRIES); do
            response=$(curl -s http://localhost:8000/metrics)
            echo "Response: $response"
            if [[ -n "$response" ]]; then
              HEALTH="OK"
              break
            fi
            echo "[$i/$MAX_RETRIES] Waiting for Karpenter to become ready..."
            sleep 10
          done
          
          # Cleanup port-forward
          kill $PF_PID || true
          
          if [ "$HEALTH" == "OK" ]; then
            echo "Karpenter is healthy ✅"
          else
            echo "Karpenter health check failed ❌"
            kubectl get pods -n kube-system -l app.kubernetes.io/name=karpenter
            kubectl logs -n kube-system -l app.kubernetes.io/name=karpenter --tail=50
            exit 1
          fi


  destroy-terraform:
    name: Destroy Terraform
    runs-on: ubuntu-latest
    env:
      CLUSTER_VERSION: ${{ github.event.inputs.cluster_version || '1.30' }}

    if: always()
    needs:
      - apply-terraform
      - install-and-validate

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::484907513542:role/github-actions-oidc-role
          aws-region: us-east-1

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Destroy Infrastructure
        working-directory: terraform/aws
        run: |
          cat <<EOF > backend_override.tf
          terraform {
            backend "s3" {
                bucket  = "zxporter-tf-state"
                key     = "${{ needs.apply-terraform.outputs.job_identifier }}/terraform.tfstate"
                region  = "us-east-1"
            }
          }
          EOF
          terraform init
          terraform destroy -auto-approve -var="cluster_name=${{ needs.apply-terraform.outputs.job_identifier }}" -var='cluster_version=${{ env.CLUSTER_VERSION }}'
