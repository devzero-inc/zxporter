syntax = "proto3";

package api.v1;

import "google/protobuf/timestamp.proto";

option go_package = "github.com/devzero-inc/zxporter/gen/api/v1;gen";
option java_multiple_files = true;
option java_package = "gen.api.v1";

// MpaService provides real-time metric streaming for in-cluster Multi-Dimensional Pod Autoscaler
service MpaService {
  // StreamWorkloadMetrics establishes a bidirectional stream where the client (Dakr)
  // subscribes to workloads, and the server (Zxporter) pushes metric updates.
  rpc StreamWorkloadMetrics(stream MpaWorkloadSubscription) returns (stream MpaStreamResponse);
}

// MpaWorkloadSubscription is a message sent by the client to update the subscription list
message MpaWorkloadSubscription {
  repeated MpaWorkloadIdentifier workloads = 1;
}

message MpaWorkloadIdentifier {
  string namespace = 1;
  string name = 2; // Workload name (e.g., deployment name)
  string kind = 3; // Workload kind (e.g., Deployment)
  map<string, string> match_labels = 4; // Selectors to match pods
}

// ContainerMetricsBatch contains a batch of metrics for containers belonging to subscribed workloads
message ContainerMetricsBatch {
  repeated ContainerMetricItem items = 1;
}

message ContainerMetricItem {
  MpaWorkloadIdentifier workload = 1; // The workload this container belongs to
  string pod_name = 2;
  string container_name = 3;

  // The metric data
  google.protobuf.Timestamp timestamp = 4;
  int64 cpu_usage_millis = 5;
  int64 memory_usage_bytes = 6;
  int64 oom_kill_count = 7; // Cumulative if available, otherwise 0
  int32 restart_count = 8;
  string last_termination_reason = 9; // e.g. "OOMKilled"

  // Resource requests and limits from pod spec (for utilization calculation)
  int64 cpu_request_millis = 10;    // CPU request in millicores
  int64 memory_request_bytes = 11;  // Memory request in bytes
  int64 cpu_limit_millis = 12;      // CPU limit in millicores
  int64 memory_limit_bytes = 13;    // Memory limit in bytes

  // Network metrics (bytes/sec, placeholder until collection extended)
  int64 network_receive_bytes_per_sec = 14;
  int64 network_transmit_bytes_per_sec = 15;

  // Disk metrics (bytes/sec, placeholder until collection extended)
  int64 disk_read_bytes_per_sec = 16;
  int64 disk_write_bytes_per_sec = 17;
}

// HistoricalMetricsSummary contains pre-aggregated percentile data from Prometheus
// for a workload over a 24-hour window. Sent on subscription start and refreshed periodically.
message HistoricalMetricsSummary {
  MpaWorkloadIdentifier workload = 1;
  repeated ContainerHistoricalMetrics containers = 2;
  google.protobuf.Timestamp window_start = 3;
  google.protobuf.Timestamp window_end = 4;
  int32 sample_count = 5;
}

message ContainerHistoricalMetrics {
  string container_name = 1;
  // CPU percentiles in millicores
  int64 cpu_p50 = 2;
  int64 cpu_p75 = 3;
  int64 cpu_p80 = 4;
  int64 cpu_p90 = 5;
  int64 cpu_p99 = 6;
  int64 cpu_p95 = 12;
  // Memory percentiles in bytes
  int64 mem_p50 = 7;
  int64 mem_p75 = 8;
  int64 mem_p80 = 9;
  int64 mem_p90 = 10;
  int64 mem_p99 = 11;
  int64 mem_p95 = 13;
  // Network percentiles in bytes/sec (placeholder until collection extended)
  int64 net_p50 = 14;
  int64 net_p80 = 15;
  int64 net_p90 = 16;
  int64 net_p95 = 17;
  int64 net_p99 = 18;
  // Disk percentiles in bytes/sec (placeholder until collection extended)
  int64 disk_p50 = 19;
  int64 disk_p80 = 20;
  int64 disk_p90 = 21;
  int64 disk_p95 = 22;
  int64 disk_p99 = 23;
}

// MpaStreamResponse wraps both real-time and historical data on the same stream
message MpaStreamResponse {
  oneof payload {
    ContainerMetricsBatch realtime_metrics = 1;
    HistoricalMetricsSummary historical_summary = 2;
  }
}
